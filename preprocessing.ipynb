{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b2/_pxlc3ws1rnfp02p32rld8tr0000gn/T/ipykernel_810/2233842345.py:87: DtypeWarning: Columns (275) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(os.path.join(directory, filename))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing: 11. Number of rows without missing data in abcd_p_demo.csv: 11857\n",
      "Deleted 11 rows with missing data from abcd_p_demo.csv.\n",
      "Deleted 864 rows with missing data from mh_p_fhx.csv.\n",
      "Missing: 320. Number of rows without missing data in mh_p_ksads_gad.csv: 11317\n",
      "Deleted 320 rows with missing data from mh_p_ksads_gad.csv.\n",
      "Missing: 56. Number of rows without missing data in mh_p_ksads_pd.csv: 11487\n",
      "Deleted 56 rows with missing data from mh_p_ksads_pd.csv.\n",
      "Missing: 203. Number of rows without missing data in mh_p_ksads_sad.csv: 11434\n",
      "Deleted 203 rows with missing data from mh_p_ksads_sad.csv.\n",
      "Missing: 729. Number of rows without missing data in mh_y_ksads_gad.csv: 10994\n",
      "Deleted 729 rows with missing data from mh_y_ksads_gad.csv.\n",
      "Missing: 679. Number of rows without missing data in mh_y_ksads_sad.csv: 11044\n",
      "Deleted 679 rows with missing data from mh_y_ksads_sad.csv.\n",
      "ksads_sad_raw_209_t\n",
      "0    7872\n",
      "1    2863\n",
      "2     309\n",
      "dtype: int64\n",
      "Missing: 11. Number of rows without missing data in mh_y_pps.csv: 11857\n",
      "Deleted 11 rows with missing data from mh_y_pps.csv.\n",
      "pps_y_ss_bother_sum\n",
      "0.0     6732\n",
      "1.0     1885\n",
      "2.0     1041\n",
      "3.0      674\n",
      "4.0      453\n",
      "5.0      318\n",
      "6.0      223\n",
      "7.0      167\n",
      "8.0      108\n",
      "9.0       80\n",
      "10.0      52\n",
      "11.0      28\n",
      "12.0      42\n",
      "13.0      19\n",
      "14.0      11\n",
      "15.0      10\n",
      "16.0       5\n",
      "17.0       3\n",
      "18.0       4\n",
      "19.0       2\n",
      "dtype: int64\n",
      "pps_y_ss_number\n",
      "0.0     4582\n",
      "1.0     1899\n",
      "2.0     1216\n",
      "3.0      900\n",
      "4.0      682\n",
      "5.0      575\n",
      "6.0      430\n",
      "7.0      348\n",
      "8.0      229\n",
      "9.0      221\n",
      "10.0     191\n",
      "11.0     146\n",
      "12.0     105\n",
      "13.0      94\n",
      "14.0      75\n",
      "15.0      58\n",
      "16.0      40\n",
      "17.0      27\n",
      "18.0      22\n",
      "19.0      10\n",
      "20.0       6\n",
      "21.0       1\n",
      "dtype: int64\n",
      "Missing: 0. Number of rows without missing data in mri_y_smr_area_dsk.csv: 11728\n",
      "Deleted 0 rows with missing data from mri_y_smr_area_dsk.csv.\n",
      "Missing: 13. Number of rows without missing data in mri_y_smr_area_dst.csv: 11715\n",
      "Deleted 13 rows with missing data from mri_y_smr_area_dst.csv.\n",
      "Missing: 0. Number of rows without missing data in mri_y_smr_sulc_dsk.csv: 11728\n",
      "Deleted 0 rows with missing data from mri_y_smr_sulc_dsk.csv.\n",
      "Missing: 13. Number of rows without missing data in mri_y_smr_sulc_dst.csv: 11715\n",
      "Deleted 13 rows with missing data from mri_y_smr_sulc_dst.csv.\n",
      "Missing: 0. Number of rows without missing data in mri_y_smr_thk_dsk.csv: 11728\n",
      "Deleted 0 rows with missing data from mri_y_smr_thk_dsk.csv.\n",
      "Missing: 13. Number of rows without missing data in mri_y_smr_thk_dst.csv: 11715\n",
      "Deleted 13 rows with missing data from mri_y_smr_thk_dst.csv.\n",
      "Missing: 0. Number of rows without missing data in mri_y_smr_vol_aseg.csv: 11728\n",
      "Deleted 0 rows with missing data from mri_y_smr_vol_aseg.csv.\n",
      "Missing: 0. Number of rows without missing data in mri_y_smr_vol_dsk.csv: 11728\n",
      "Deleted 0 rows with missing data from mri_y_smr_vol_dsk.csv.\n",
      "Missing: 13. Number of rows without missing data in mri_y_smr_vol_dst.csv: 11715\n",
      "Deleted 13 rows with missing data from mri_y_smr_vol_dst.csv.\n",
      "Missing: 2. Number of rows without missing data in mri_y_dti_fa_is_at.csv: 11102\n",
      "Deleted 2 rows with missing data from mri_y_dti_fa_is_at.csv.\n",
      "Missing: 2. Number of rows without missing data in mri_y_dti_ld_is_at.csv: 11102\n",
      "Deleted 2 rows with missing data from mri_y_dti_ld_is_at.csv.\n",
      "Missing: 2. Number of rows without missing data in mri_y_dti_md_is_at.csv: 11102\n",
      "Deleted 2 rows with missing data from mri_y_dti_md_is_at.csv.\n",
      "Missing: 2. Number of rows without missing data in mri_y_dti_td_is_at.csv: 11102\n",
      "Deleted 2 rows with missing data from mri_y_dti_td_is_at.csv.\n",
      "Missing: 3517. Number of rows without missing data in RVI_groups.csv: 8242\n",
      "Deleted 3517 rows with missing data from RVI_groups.csv.\n",
      "Deleted 5524 rows with missing data from merged_data.csv.\n",
      "Squeeky clean, ood, outliers: 1424 4764 160\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Get the current working directory\n",
    "directory = os.getcwd()\n",
    "\n",
    "# Initialize an empty list to store DataFrames for each CSV file\n",
    "dataframes = []\n",
    "\n",
    "csv_files_order = [\n",
    "    \"abcd_p_demo.csv\",\n",
    "    \"mh_p_fhx.csv\",\n",
    "    \"mh_p_ksads_gad.csv\",\n",
    "    \"mh_p_ksads_pd.csv\",\n",
    "    \"mh_p_ksads_sad.csv\",\n",
    "    \"mh_y_ksads_gad.csv\",\n",
    "    \"mh_y_ksads_sad.csv\",\n",
    "    \"mh_y_pps.csv\",\n",
    "    \"mri_y_smr_area_dsk.csv\",\n",
    "    \"mri_y_smr_area_dst.csv\",\n",
    "    \"mri_y_smr_sulc_dsk.csv\",\n",
    "    \"mri_y_smr_sulc_dst.csv\",\n",
    "    \"mri_y_smr_thk_dsk.csv\",\n",
    "    \"mri_y_smr_thk_dst.csv\",\n",
    "    \"mri_y_smr_vol_aseg.csv\",\n",
    "    \"mri_y_smr_vol_dsk.csv\",\n",
    "    \"mri_y_smr_vol_dst.csv\",\n",
    "    \"mri_y_dti_fa_is_at.csv\",\n",
    "    \"mri_y_dti_ld_is_at.csv\",\n",
    "    \"mri_y_dti_md_is_at.csv\",\n",
    "    \"mri_y_dti_td_is_at.csv\",\n",
    "    \"RVI_groups.csv\"\n",
    "]\n",
    "\n",
    "def count_mri_missing_data(df, filename) -> str:\n",
    "    # Count the number of rows with missing data\n",
    "    missing_rows_count = df.isnull().any(axis=1).sum()\n",
    "    total = len(df)\n",
    "    return(f\"Missing: {missing_rows_count}. Number of rows without missing data in {filename}: {total - missing_rows_count}\")\n",
    "\n",
    "\n",
    "def drop_rows_with_missing_data(df, filename):\n",
    "    # Drop rows with any missing value in any column\n",
    "    og_length = len(df)\n",
    "\n",
    "    df.replace('NA', np.nan, inplace=True)\n",
    "    df = df.dropna(axis=0)\n",
    "    \n",
    "    # Print info about the deleted rows\n",
    "    deleted_rows_count = og_length - len(df)\n",
    "    print(f\"Deleted {deleted_rows_count} rows with missing data from {filename}.\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def delete_specific_columns(df, columns):\n",
    "    # Delete specific columns from the DataFrame\n",
    "    df = df.drop(columns=columns, axis=1)\n",
    "    return df\n",
    "\n",
    "# Define criteria for in distribution vs out of distribution\n",
    "def typical(row):\n",
    "    if row['famhx_ss_fath_prob_vs_p'] == 0 and row['famhx_ss_moth_prob_vs_p'] == 0 and row['famhx_ss_fath_prob_nrv_p'] == 0 and row['famhx_ss_moth_prob_nrv_p'] == 0 \\\n",
    "        and row['ksads_gad_raw_271_p'] < 1 and row['ksads_gad_raw_273_p'] == 0 \\\n",
    "        and row['ksads_pd_raw_176_p'] < 1 and row['ksads_pd_raw_178_p'] == 0 \\\n",
    "        and row['ksads_sad_raw_209_p'] < 1 and row['ksads_sad_raw_211_p'] == 0 \\\n",
    "        and row['ksads_gad_raw_271_t'] < 1 and row['ksads_gad_raw_273_t'] == 0 \\\n",
    "        and row['ksads_sad_raw_209_t'] < 1 and row['ksads_sad_raw_211_t'] == 0 \\\n",
    "        and row['group_last_final'] == 5 \\\n",
    "        and row['pps_y_ss_number'] <= 1 and (row['pps_y_ss_bother_sum'] <= 1 or (row['pps_y_ss_number'] <= 1 and pd.isna(row['pps_y_ss_bother_sum']))):\n",
    "\n",
    "        return 0 #squeeky clean\n",
    "    \n",
    "    elif row['pps_y_ss_number'] > 2 and row['pps_y_ss_bother_sum'] > 2 and row['group_last_final'] == 1:\n",
    "        \n",
    "        return 2 #outlier\n",
    "    \n",
    "    else:\n",
    "        return 1 #\"typical\"\n",
    "\n",
    "\n",
    "# Iterate through each CSV file in the directory\n",
    "for filename in csv_files_order:\n",
    "    if filename in os.listdir(directory):\n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(os.path.join(directory, filename))\n",
    "        \n",
    "        # Filter rows with \"baseline\" for the event name column\n",
    "        if 'eventname' in df.columns:\n",
    "            df = df[df['eventname'].str.startswith(\"baseline\")]\n",
    "\n",
    "        # Select columns you want to keep and check amounts of usable data\n",
    "        if 'abcd_p_demo.csv' in filename:\n",
    "            df = df[['src_subject_id', 'eventname', 'demo_brthdat_v2']] \n",
    "            print(count_mri_missing_data(df, filename)) \n",
    "            df = drop_rows_with_missing_data(df, filename)\n",
    "        \n",
    "        if 'mh_p_cbcl.csv' in filename:\n",
    "            exit\n",
    "\n",
    "        if 'RVI_groups.csv' in filename:\n",
    "            df = df.rename(columns={'ID': 'src_subject_id'})\n",
    "            df = df[['src_subject_id', 'group_last_final']]\n",
    "            print(count_mri_missing_data(df, filename))\n",
    "            df = drop_rows_with_missing_data(df, filename)\n",
    "        \n",
    "        if 'mh_p_fhx.csv' in filename:\n",
    "            df = df[['src_subject_id','famhx_ss_fath_prob_vs_p', 'famhx_ss_moth_prob_vs_p','famhx_ss_fath_prob_nrv_p','famhx_ss_moth_prob_nrv_p']]  \n",
    "            df = drop_rows_with_missing_data(df, filename)\n",
    "\n",
    "\n",
    "        if 'mh_p_ksads_gad.csv' in filename:\n",
    "            df = df[['src_subject_id', 'ksads_gad_raw_271_p', 'ksads_gad_raw_273_p']]\n",
    "            print(count_mri_missing_data(df, filename))\n",
    "            df = drop_rows_with_missing_data(df, filename)\n",
    "\n",
    "        if 'mh_p_ksads_pd.csv' in filename:\n",
    "            df = df[['src_subject_id', 'ksads_pd_raw_176_p', 'ksads_pd_raw_178_p']]\n",
    "            print(count_mri_missing_data(df, filename))\n",
    "            df = drop_rows_with_missing_data(df, filename)\n",
    "\n",
    "        if 'mh_p_ksads_sad.csv' in filename:\n",
    "            df = df[['src_subject_id', 'ksads_sad_raw_209_p', 'ksads_sad_raw_211_p']]\n",
    "            print(count_mri_missing_data(df, filename))\n",
    "            df = drop_rows_with_missing_data(df, filename)\n",
    "\n",
    "        if 'mh_y_ksads_gad.csv' in filename:\n",
    "            df = df[['src_subject_id', 'ksads_gad_raw_271_t', 'ksads_gad_raw_273_t']]\n",
    "            print(count_mri_missing_data(df, filename))\n",
    "            df = drop_rows_with_missing_data(df, filename)\n",
    "        \n",
    "        if 'mh_y_ksads_sad.csv' in filename:\n",
    "            df = df[['src_subject_id', 'ksads_sad_raw_209_t', 'ksads_sad_raw_211_t']]\n",
    "            print(count_mri_missing_data(df, filename))\n",
    "            df = drop_rows_with_missing_data(df, filename)\n",
    "            sum_per_entry = df.groupby('ksads_sad_raw_209_t').size()\n",
    "            print(sum_per_entry)\n",
    "        \n",
    "        if 'mh_y_pps.csv' in filename:\n",
    "            df = df[['src_subject_id', 'pps_y_ss_number', 'pps_y_ss_bother_sum']]\n",
    "            mask = (df['pps_y_ss_number'] == 0) & (df['pps_y_ss_bother_sum'].isna())\n",
    "            df.loc[mask, 'pps_y_ss_bother_sum'] = 0\n",
    "            print(count_mri_missing_data(df, filename))\n",
    "            df = drop_rows_with_missing_data(df, filename)\n",
    "            sum_per_entry = df.groupby('pps_y_ss_bother_sum').size()\n",
    "            print(sum_per_entry)\n",
    "            sum_per_entry = df.groupby('pps_y_ss_number').size()\n",
    "            print(sum_per_entry)\n",
    "        \n",
    "        if 'mri_y_dti_fa_is_at.csv' in filename:\n",
    "            print(count_mri_missing_data(df, filename))\n",
    "            df = drop_rows_with_missing_data(df, filename)\n",
    "        \n",
    "        if 'mri_y_dti_ld_is_at.csv' in filename:\n",
    "            print(count_mri_missing_data(df, filename))\n",
    "            df = drop_rows_with_missing_data(df, filename)\n",
    "\n",
    "        if 'mri_y_dti_md_is_at.csv' in filename:\n",
    "            print(count_mri_missing_data(df, filename))\n",
    "            df = drop_rows_with_missing_data(df, filename)\n",
    "\n",
    "        if 'mri_y_dti_td_is_at.csv' in filename:\n",
    "            print(count_mri_missing_data(df, filename))\n",
    "            df = drop_rows_with_missing_data(df, filename)\n",
    "\n",
    "        if 'mri_y_smr_area_dsk.csv' in filename:\n",
    "            print(count_mri_missing_data(df, filename))\n",
    "            df = drop_rows_with_missing_data(df, filename)\n",
    "\n",
    "        if 'mri_y_smr_area_dst.csv' in filename:\n",
    "            print(count_mri_missing_data(df, filename))\n",
    "            df = drop_rows_with_missing_data(df, filename)\n",
    "\n",
    "        if 'mri_y_smr_sulc_dsk.csv' in filename:\n",
    "            print(count_mri_missing_data(df, filename))\n",
    "            df = drop_rows_with_missing_data(df, filename)\n",
    "\n",
    "        if 'mri_y_smr_sulc_dst.csv' in filename:\n",
    "            print(count_mri_missing_data(df, filename))\n",
    "            df = drop_rows_with_missing_data(df, filename)\n",
    "\n",
    "        if 'mri_y_smr_thk_dsk.csv' in filename:\n",
    "            print(count_mri_missing_data(df, filename))\n",
    "            df = drop_rows_with_missing_data(df, filename)\n",
    "\n",
    "        if 'mri_y_smr_thk_dst.csv' in filename:\n",
    "            print(count_mri_missing_data(df, filename))\n",
    "            df = drop_rows_with_missing_data(df, filename)\n",
    "\n",
    "        if 'mri_y_smr_vol_aseg.csv' in filename:\n",
    "            df = delete_specific_columns(df, columns=['smri_vol_scs_lesionlh', 'smri_vol_scs_lesionrh'])\n",
    "            print(count_mri_missing_data(df, filename))\n",
    "            df = drop_rows_with_missing_data(df, filename)\n",
    "\n",
    "        if 'mri_y_smr_vol_dsk.csv' in filename:\n",
    "            print(count_mri_missing_data(df, filename))\n",
    "            df = drop_rows_with_missing_data(df, filename)\n",
    "\n",
    "        if 'mri_y_smr_vol_dst.csv' in filename:\n",
    "           print(count_mri_missing_data(df, filename))\n",
    "           df = drop_rows_with_missing_data(df, filename)\n",
    "\n",
    "        \n",
    "        # Append the processed DataFrame to the list\n",
    "       \n",
    "        dataframes.append(df)\n",
    "        \n",
    "        \n",
    "\n",
    "#Concatenate all DataFrames into a single DataFrame\n",
    "merged_data = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Group by subject ID and aggregate the rows using a function (e.g., first)\n",
    "merged_data = merged_data.groupby('src_subject_id').first().reset_index()\n",
    "\n",
    "\n",
    "\n",
    "merged_data = drop_rows_with_missing_data(merged_data, 'merged_data.csv')\n",
    "\n",
    "merged_data = merged_data.drop(columns=['src_subject_id', 'eventname'])\n",
    "# Calculate mean and standard deviation of 'pps_y_ss_number' column\n",
    "\n",
    "# numpy estima lambda value to find mean and standard deviation. \n",
    "# mean_pps = merged_data['pps_y_ss_number'].mean()\n",
    "# std_pps = merged_data['pps_y_ss_number'].std()\n",
    "\n",
    "# mean_pps_bother = merged_data['pps_y_ss_bother_sum'].mean()\n",
    "# std_pps_bother = merged_data['pps_y_ss_bother_sum'].std()\n",
    "\n",
    "# print(mean_pps, std_pps, mean_pps_bother, std_pps_bother)\n",
    "\n",
    "# Define threshold for distribution\n",
    "# in_threshold = mean_pps - 0.5 * std_pps\n",
    "# in_threshold_bother = mean_pps_bother - 0.5 * std_pps_bother\n",
    "\n",
    "\n",
    "# Apply the criteria to each row and create a new column\n",
    "merged_data['Distribution Status'] = merged_data.apply(typical, axis=1)\n",
    "\n",
    "\n",
    "# Calculate the sum of all values in the \"Distribution Status\" column\n",
    "\n",
    "squeeky = (merged_data['Distribution Status'] == 0).sum()\n",
    "ood_total = (merged_data['Distribution Status'] == 1).sum()\n",
    "outlier = (merged_data['Distribution Status'] == 2).sum()\n",
    "# Add a new column with the sum as its value for all rows\n",
    "# merged_data['Total Distribution Status'] = ood_total\n",
    "\n",
    "print(\"Squeeky clean, ood, outliers:\", squeeky, ood_total, outlier)\n",
    "\n",
    "\n",
    "# # Write the merged DataFrame to a new CSV file\n",
    "merged_data.to_csv(os.path.join(directory, 'merged_data.csv'), index=False)\n",
    "\n",
    "\n",
    "# # Now you have all your data merged into one DataFrame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
